{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crawling(ticker, date):\n",
    "    for j in range(1,51):\n",
    "        try:\n",
    "            # 사이트 접속\n",
    "            custom_header = {\n",
    "                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "                \"referer\": \"https://finance.naver.com/\"\n",
    "            }\n",
    "            url = \"https://finance.naver.com/item/sise_time.naver?code={ticker}&thistime={date}&page={j}\".format(ticker = ticker, date = date, j=j)\n",
    "            response = requests.get(url, headers=custom_header)\n",
    "            response.raise_for_status()\n",
    "            # time.sleep(0.2)\n",
    "        except:\n",
    "            return makeDf(lst, soup, ticker, date)\n",
    "        # 텍스트 추출\n",
    "        response.text\n",
    "\n",
    "        # html로 파싱//\n",
    "        soup = bs(response.text, \"html.parser\")\n",
    "\n",
    "        # 마지막 페이지 찾기\n",
    "        find_page = soup.findAll(\"a\") \n",
    "        href = []\n",
    "        for link in find_page:\n",
    "            href = link.attrs['href']\n",
    "        last_page = href[-2:]\n",
    "\n",
    "        # 반복문 빠져나가기\n",
    "        if int(last_page) < j :\n",
    "            return makeDf(lst, soup, ticker, date)\n",
    "\n",
    "        # if not column:\n",
    "        #    th_list = soup.find_all(\"th\")\n",
    "        #    for i in range(7):\n",
    "        #        column.append(th_list[i].get_text())\n",
    "        #    lst.append(column)\n",
    "\n",
    "        td_list = soup.find_all(\"td\") # 통괄\n",
    "        # span_list = soup.find_all(\"span\", class_=\"tah p10 gray03\") #채결 시각\n",
    "        # span1_list = soup.find_all(\"span\", class_=\"tah p11\") # 채결가, 매도, 매수, 거래량, 변동량\n",
    "        # span2_list = soup.find_all(\"span\", class_=\"tah p11 nv01\") # 전일비\n",
    "\n",
    "\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                contents = []\n",
    "                contents.append(td_list[7*i+1].text) # 채결 시각\n",
    "                contents.append(td_list[7*i+2].text) #채결가\n",
    "                contents.append(re.findall(r'\\d?,?\\d+', td_list[7*i+3].text)[0]) # 전일비\n",
    "                contents.append(td_list[7*i+4].text) # 매도호가\n",
    "                contents.append(td_list[7*i+5].text) # 매수호가\n",
    "                contents.append(td_list[7*i+6].text) # 거래량\n",
    "                contents.append(td_list[7*i+7].text) # 변동량\n",
    "                lst.append(contents)\n",
    "            except:\n",
    "                return makeDf(lst, soup, ticker, date)\n",
    "            \n",
    "        for i in range(5):\n",
    "            try:\n",
    "                contents = []\n",
    "                contents.append(td_list[7*i+39].text) # 채결 시각\n",
    "                contents.append(td_list[7*i+40].text) #채결가\n",
    "                contents.append(re.findall(r'\\d?,?\\d+', td_list[7*i+41].text)[0]) # 전일비\n",
    "                contents.append(td_list[7*i+42].text) # 매도호가\n",
    "                contents.append(td_list[7*i+43].text) # 매수호가\n",
    "                contents.append(td_list[7*i+44].text) # 거래량\n",
    "                contents.append(td_list[7*i+45].text) # 변동량\n",
    "                lst.append(contents)\n",
    "            except:\n",
    "                return makeDf(lst, soup, ticker, date)\n",
    "            \n",
    "def makeDf(lst, soup, ticker, date):\n",
    "    df = pd.DataFrame(lst)\n",
    "    try:\n",
    "        th_list = soup.find_all(\"th\")\n",
    "        column = []\n",
    "        for i in range(len(th_list)):\n",
    "            column.append(th_list[i].text)\n",
    "        df.columns=column\n",
    "        return saveDf(df, ticker, date)\n",
    "    except :\n",
    "        return saveDf(df, ticker, date)\n",
    "\n",
    "\n",
    "# 추출 데이터 csv 저장\n",
    "def saveDf(df, ticker, date):\n",
    "    try:\n",
    "        df.to_csv('./csvsource/{ticker}/{date}.csv'.format(ticker=ticker, date=date), index=False, encoding='utf-8-sig')\n",
    "    except:\n",
    "        try:\n",
    "            os.mkdir('csvsource')\n",
    "            os.mkdir('csvsource/{ticker}'.format(ticker = ticker))\n",
    "            df.to_csv('./csvsource/{ticker}/{date}.csv'.format(ticker=ticker, date=date), index=False, encoding='utf-8-sig')\n",
    "        except:\n",
    "            os.mkdir('csvsource/{ticker}'.format(ticker = ticker))\n",
    "            df.to_csv('./csvsource/{ticker}/{date}.csv'.format(ticker=ticker, date=date), index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "# 기존 하드웨어를 가지고 SI, AI에 투자하는 기업 (다소 SI에 편향되어 있는 기업도 있음)\n",
    "    # 006400 삼성 SDI\n",
    "    # 018260 삼성 SDS <- 미라콤아이앤씨의 모회사\n",
    "    # 022100 포스코 DX\n",
    "    # 003550 LG <- LG CNS로 하고 싶었으나 독점 운영하다가 징계 먹고 최근에 분할되서 상장되려면 2020.4에 사모펀드로 강제 매각됨 국내 정식 상장은 2025년 예상\n",
    "    # 307950 현대오토에버\n",
    "    # 034730 SK inc. <- sk 텔레콤, sk C&C 지주회사\n",
    "    # 286940 롯데이노베이트\n",
    "    # 272210 한화시스템\n",
    "    # 035510 신세계 I&C\n",
    "    # 069960 현대백화점 <- 현대 it&e가 현대백화점의 IT 전문 기업\n",
    "\n",
    "# 대규모 정보를 기반으로 AI에 투자하는 기업\n",
    "    # 005930 삼성전자 \n",
    "    # 035420 네이버\n",
    "    # 003550 LG <- LG CNS로 하고 싶었으나 독점 운영하다가 징계 먹고 최근에 분할되서 상장되려면 2020.4에 사모펀드로 강제 매각됨 국내 정식 상장은 2025년 예상\n",
    "    # 034730 SK inc. <- sk 텔레콤, sk C&C 지주회사\n",
    "    # 035720 카카오\n",
    "    # 000660 SK 하이닉스\n",
    "    # 030200 KT\n",
    "    # 005380 현대자동차\n",
    "    # 005490 포스코\n",
    "\n",
    "code = ['006400', '018260', '022100', '307950', '286940', '272210', '035510', '069960', '005930', '035420', '003550', '034730', '035720', '000660', '030200', '005380', '005490'] # 종목코드\n",
    "dates = ['202409051800', '202409061800', '202409091800'] # 날짜 \n",
    "\n",
    "for date in dates:\n",
    "    for ticker in code:\n",
    "        # 데이터 저장할 리스트\n",
    "        lst = []\n",
    "        df = crawling(ticker, date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
